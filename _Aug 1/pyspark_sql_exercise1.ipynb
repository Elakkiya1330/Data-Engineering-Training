{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Scenario: Employee Work Data for a Tech Company\n",
        "##Step 1: Prepare Data in PySpark"
      ],
      "metadata": {
        "id": "wRaV2eRG-91i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "yytRZsP5-2tR"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQLExercise1\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "data = [\n",
        "Row(EmpID=101, Name=\"Ravi\", Department=\"Engineering\", Project=\"AI Engine\",\n",
        "Salary=95000, HoursPerWeek=42),\n",
        "Row(EmpID=102, Name=\"Sneha\", Department=\"Engineering\", Project=\"Data Platform\",\n",
        "Salary=87000, HoursPerWeek=45),\n",
        "Row(EmpID=103, Name=\"Kabir\", Department=\"Marketing\", Project=\"Product Launch\",\n",
        "Salary=65000, HoursPerWeek=40),\n",
        "Row(EmpID=104, Name=\"Anita\", Department=\"Sales\", Project=\"Client Outreach\",\n",
        "Salary=70000, HoursPerWeek=38),\n",
        "Row(EmpID=105, Name=\"Divya\", Department=\"Engineering\", Project=\"AI Engine\",\n",
        "Salary=99000, HoursPerWeek=48),\n",
        "Row(EmpID=106, Name=\"Amit\", Department=\"Marketing\", Project=\"Social Media\",\n",
        "Salary=62000, HoursPerWeek=35),\n",
        "Row(EmpID=107, Name=\"Priya\", Department=\"HR\", Project=\"Policy Revamp\",\n",
        "Salary=58000, HoursPerWeek=37),\n",
        "Row(EmpID=108, Name=\"Manav\", Department=\"Sales\", Project=\"Lead Gen\", Salary=73000,\n",
        "HoursPerWeek=41),\n",
        "Row(EmpID=109, Name=\"Neha\", Department=\"Engineering\", Project=\"Security Suite\",\n",
        "Salary=91000, HoursPerWeek=46),\n",
        "Row(EmpID=110, Name=\"Farah\", Department=\"HR\", Project=\"Onboarding\", Salary=60000,\n",
        "HoursPerWeek=36)\n",
        "]\n",
        "df = spark.createDataFrame(data)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDDr-105_HNH",
        "outputId": "0e44d2e8-187e-4623-e328-83a8b87b763f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Create Views\n",
        "##Create a Local Temp View"
      ],
      "metadata": {
        "id": "PaK3qy2__ecT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"employees_local\")"
      ],
      "metadata": {
        "id": "OQIrTvRn_aNU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create a Global Temp View"
      ],
      "metadata": {
        "id": "E1K7w9KT_m4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceGlobalTempView(\"employees_global\")"
      ],
      "metadata": {
        "id": "nsaZewxD_qc0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part A: Exercises on Local View ( employees_local )"
      ],
      "metadata": {
        "id": "OtaPA5un_tzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. List all employees working on the \"AI Engine\" project.\n",
        "spark.sql(\"select * from employees_local where Project = 'AI Engine'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVESLDr__w01",
        "outputId": "4d7261dc-fb5d-4fa4-8718-cf863a25363a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------+------+------------+\n",
            "|EmpID| Name| Department|  Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------+------+------------+\n",
            "|  101| Ravi|Engineering|AI Engine| 95000|          42|\n",
            "|  105|Divya|Engineering|AI Engine| 99000|          48|\n",
            "+-----+-----+-----------+---------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show all employees from the \"Marketing\" department with salaries greater than 60,000.\n",
        "spark.sql(\"select * from employees_local where Department = 'Marketing' and Salary > 60000\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1uIuWXqASHN",
        "outputId": "770ed737-d541-4c1f-a447-cb3b344d2d9e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+--------------+------+------------+\n",
            "|EmpID| Name|Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+----------+--------------+------+------------+\n",
            "|  103|Kabir| Marketing|Product Launch| 65000|          40|\n",
            "|  106| Amit| Marketing|  Social Media| 62000|          35|\n",
            "+-----+-----+----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Calculate the average salary for each department.\n",
        "spark.sql(\"select Department ,  avg(Salary) from employees_local group by Department\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRbLrOw5Akfc",
        "outputId": "48a6c1a4-d9ce-4f91-c82c-1d5ca6829787"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "| Department|avg(Salary)|\n",
            "+-----------+-----------+\n",
            "|      Sales|    71500.0|\n",
            "|Engineering|    93000.0|\n",
            "|  Marketing|    63500.0|\n",
            "|         HR|    59000.0|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. List the top 3 highest paid employees overall.\n",
        "spark.sql(\"select Name, Salary from employees_local order by Salary Desc limit 3\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g50H0zEZA0bd",
        "outputId": "724f266e-667b-4d62-fc26-c6d0e12a4aaa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "| Name|Salary|\n",
            "+-----+------+\n",
            "|Divya| 99000|\n",
            "| Ravi| 95000|\n",
            "| Neha| 91000|\n",
            "+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Find employees who work more than 40 hours per week.\n",
        "spark.sql(\"select * from employees_local where HoursPerWeek > 40\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spjeR7sRBKxb",
        "outputId": "58e5a834-2c88-4c4c-a526-47d4e070611a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|EmpID| Name| Department|       Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "|  101| Ravi|Engineering|     AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering| Data Platform| 87000|          45|\n",
            "|  105|Divya|Engineering|     AI Engine| 99000|          48|\n",
            "|  108|Manav|      Sales|      Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering|Security Suite| 91000|          46|\n",
            "+-----+-----+-----------+--------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Group by project and display the number of employees per project.\n",
        "spark.sql(\"select Project, count(*) as no_emp from employees_local group by Project\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4y8ElK9BYHN",
        "outputId": "cab7bd13-9a3c-43c3-9a50-947d59f598e7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+------+\n",
            "|        Project|no_emp|\n",
            "+---------------+------+\n",
            "|  Data Platform|     1|\n",
            "|      AI Engine|     2|\n",
            "| Product Launch|     1|\n",
            "|Client Outreach|     1|\n",
            "| Security Suite|     1|\n",
            "|  Policy Revamp|     1|\n",
            "|       Lead Gen|     1|\n",
            "|   Social Media|     1|\n",
            "|     Onboarding|     1|\n",
            "+---------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Drop the local view. Try querying again — what happens?\n",
        "spark.catalog.dropTempView(\"employees_local\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccd6b_9MChbi",
        "outputId": "17da04b6-946b-4cbe-c880-ee5a1a10f6fc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# error occurs\n",
        "spark.sql(\"select * from employees_local \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "gHpRdaUYC7Gf",
        "outputId": "317f6af5-6a31-4476-ab44-47e0d223bea4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `employees_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [employees_local], [], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3911158591.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from employees_local \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                 )\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `employees_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [employees_local], [], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part B: Exercises on Global View ( employees_global )"
      ],
      "metadata": {
        "id": "eI0Wl_c-DlaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Retrieve all \"HR\" employees working fewer than 38 hours/week.\n",
        "spark.sql(\"select * from global_temp.employees_global where Department = 'HR' and HoursPerWeek < 38\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9YNpnA5Dfom",
        "outputId": "15dc9e88-b571-4691-bc56-66a0b80584eb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+-------------+------+------------+\n",
            "|EmpID| Name|Department|      Project|Salary|HoursPerWeek|\n",
            "+-----+-----+----------+-------------+------+------------+\n",
            "|  107|Priya|        HR|Policy Revamp| 58000|          37|\n",
            "|  110|Farah|        HR|   Onboarding| 60000|          36|\n",
            "+-----+-----+----------+-------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Calculate the total salary payout for each department.\n",
        "spark.sql(\"select Department, sum(Salary) from global_temp.employees_global group by Department\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqCeWl-OEkiJ",
        "outputId": "6afe6810-00f8-449e-df06-bb38703686a7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----------+\n",
            "| Department|sum(Salary)|\n",
            "+-----------+-----------+\n",
            "|      Sales|     143000|\n",
            "|Engineering|     372000|\n",
            "|  Marketing|     127000|\n",
            "|         HR|     118000|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. For each employee, add a derived column Status :\n",
        "# If HoursPerWeek > 45 → 'Overworked'\n",
        "# Otherwise → 'Normal'\n",
        "from pyspark.sql.functions import when\n",
        "df_status = df.withColumn('Status', when(df.HoursPerWeek > 45, 'Overworked').otherwise('Normal'))\n",
        "df_status.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UmOmUFaEy65",
        "outputId": "bb9d63bf-adf2-4b78-e2a1-eb6b1800a07a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|    Status|\n",
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|    Normal|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|    Normal|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|    Normal|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|    Normal|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|Overworked|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|    Normal|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|    Normal|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|    Normal|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|Overworked|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|    Normal|\n",
            "+-----+-----+-----------+---------------+------+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Count the total number of employees working on each \"Project\" .\n",
        "spark.sql(\"select Project, count(*) as no_emp from global_temp.employees_global group by Project\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEdfmSX3FwSZ",
        "outputId": "9a6ba637-8b5d-48a2-9bf3-e902a111eab8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+------+\n",
            "|        Project|no_emp|\n",
            "+---------------+------+\n",
            "|  Data Platform|     1|\n",
            "|      AI Engine|     2|\n",
            "| Product Launch|     1|\n",
            "|Client Outreach|     1|\n",
            "| Security Suite|     1|\n",
            "|  Policy Revamp|     1|\n",
            "|       Lead Gen|     1|\n",
            "|   Social Media|     1|\n",
            "|     Onboarding|     1|\n",
            "+---------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. List employees whose salary is above the average salary in their department.\n",
        "from pyspark.sql.functions import col\n",
        "avg_salary = df.groupBy(\"Department\").avg(\"Salary\").withColumnRenamed(\"avg(Salary)\", \"AvgSalary\")\n",
        "result = df.join(avg_salary, \"Department\").filter(col(\"Salary\") > col(\"AvgSalary\")).select(\"EmpID\", \"Name\", \"Department\", \"Salary\")\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cZwgUZ4F7S5",
        "outputId": "f1b749a4-6221-4211-9b11-586136055648"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+------+\n",
            "|EmpID| Name| Department|Salary|\n",
            "+-----+-----+-----------+------+\n",
            "|  101| Ravi|Engineering| 95000|\n",
            "|  105|Divya|Engineering| 99000|\n",
            "|  103|Kabir|  Marketing| 65000|\n",
            "|  108|Manav|      Sales| 73000|\n",
            "|  110|Farah|         HR| 60000|\n",
            "+-----+-----+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Open a new Spark session and query \"global_temp.employees_global\" from there.\n",
        "new_spark = SparkSession.builder.appName(\"SparkSQL_new\").getOrCreate()"
      ],
      "metadata": {
        "id": "Js5YWIKuJ9-o"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from global_temp.employees_global \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoxtt-yDKODn",
        "outputId": "4a66d3d0-6850-4c4c-8bd4-52801f396ce2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|EmpID| Name| Department|        Project|Salary|HoursPerWeek|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "|  101| Ravi|Engineering|      AI Engine| 95000|          42|\n",
            "|  102|Sneha|Engineering|  Data Platform| 87000|          45|\n",
            "|  103|Kabir|  Marketing| Product Launch| 65000|          40|\n",
            "|  104|Anita|      Sales|Client Outreach| 70000|          38|\n",
            "|  105|Divya|Engineering|      AI Engine| 99000|          48|\n",
            "|  106| Amit|  Marketing|   Social Media| 62000|          35|\n",
            "|  107|Priya|         HR|  Policy Revamp| 58000|          37|\n",
            "|  108|Manav|      Sales|       Lead Gen| 73000|          41|\n",
            "|  109| Neha|Engineering| Security Suite| 91000|          46|\n",
            "|  110|Farah|         HR|     Onboarding| 60000|          36|\n",
            "+-----+-----+-----------+---------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bonus Challenges"
      ],
      "metadata": {
        "id": "rhUl4GqiKXkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Use a window function to assign rank to employees within each department based on salary.\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank, desc\n",
        "windowSpec = Window.partitionBy(\"Department\").orderBy(desc(\"Salary\"))\n",
        "ranked_df = df.withColumn(\"Rank\", rank().over(windowSpec))\n",
        "ranked_df.select(\"EmpID\", \"Name\", \"Department\", \"Salary\", \"Rank\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohWjI3fFKZLh",
        "outputId": "543e7152-efec-45e2-911a-de15aa5224ed"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----------+------+----+\n",
            "|EmpID| Name| Department|Salary|Rank|\n",
            "+-----+-----+-----------+------+----+\n",
            "|  105|Divya|Engineering| 99000|   1|\n",
            "|  101| Ravi|Engineering| 95000|   2|\n",
            "|  109| Neha|Engineering| 91000|   3|\n",
            "|  102|Sneha|Engineering| 87000|   4|\n",
            "|  110|Farah|         HR| 60000|   1|\n",
            "|  107|Priya|         HR| 58000|   2|\n",
            "|  103|Kabir|  Marketing| 65000|   1|\n",
            "|  106| Amit|  Marketing| 62000|   2|\n",
            "|  108|Manav|      Sales| 73000|   1|\n",
            "|  104|Anita|      Sales| 70000|   2|\n",
            "+-----+-----+-----------+------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Create another view (local or global) that only contains \"Engineering\" employees.\n",
        "df.filter(df.Department == \"Engineering\").createOrReplaceGlobalTempView(\"engineering_employees_global\")"
      ],
      "metadata": {
        "id": "FugJFV_-Kn9u"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create a SQL view that filters out all employees working < 38 hours and saves it as \"active_employees\"\n",
        "active_df = df.filter(df.HoursPerWeek < 38)\n",
        "active_df.createOrReplaceTempView(\"active_employees\")\n",
        "spark.sql(\"SELECT * FROM active_employees\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGB7_vUHKrv7",
        "outputId": "a1a0f5e2-2512-4934-a029-06b63e1c365b"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+----------+-------------+------+------------+\n",
            "|EmpID| Name|Department|      Project|Salary|HoursPerWeek|\n",
            "+-----+-----+----------+-------------+------+------------+\n",
            "|  106| Amit| Marketing| Social Media| 62000|          35|\n",
            "|  107|Priya|        HR|Policy Revamp| 58000|          37|\n",
            "|  110|Farah|        HR|   Onboarding| 60000|          36|\n",
            "+-----+-----+----------+-------------+------+------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}