{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Spark SQL Exercise Set – Product Orders Analytics\n",
        "Preparation Instructions\n",
        "1. Create a PySpark DataFrame with the following schema:\n",
        "OrderID (int)\n",
        "CustomerName (string)\n",
        "Product (string)\n",
        "Category (string)\n",
        "Quantity (int)\n",
        "UnitPrice (int)\n",
        "OrderDate (string in YYYY-MM-DD format)\n",
        "2. Sample at least 12 rows across multiple categories:\n",
        "\"Electronics\" , \"Clothing\" , \"Furniture\" , \"Books\"\n",
        "3. Create:\n",
        "A local temporary view: \"orders_local\"\n",
        "A global temporary view: \"orders_global\""
      ],
      "metadata": {
        "id": "wO53zAG6Wh34"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJiom_tvVrK7",
        "outputId": "8afcf280-92b9-4a99-b61a-87552c62bcbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+------------+-----------+--------+---------+----------+\n",
            "|OrderID|CustomerName|     Product|   Category|Quantity|UnitPrice| OrderDate|\n",
            "+-------+------------+------------+-----------+--------+---------+----------+\n",
            "|    101|     Abinaya|  Smartphone|Electronics|       2|    15000|2025-07-01|\n",
            "|    102|    Sereesha|     T-Shirt|   Clothing|       3|      500|2025-06-02|\n",
            "|    103|       Charu|      Laptop|Electronics|       1|    60000|2025-05-03|\n",
            "|    104|      Harish|        Sofa|  Furniture|       1|    25000|2025-07-04|\n",
            "|    105|    Elakkiya|       Novel|      Books|       2|      300|2025-07-05|\n",
            "|    106|     Kashifa|      Tablet|Electronics|       1|    20000|2025-06-06|\n",
            "|    107|        Roja|       Dress|   Clothing|       2|     1500|2023-01-07|\n",
            "|    108|      Harish|   Bookshelf|  Furniture|       1|     8000|2025-07-08|\n",
            "|    109|    Varshini|  Headphones|Electronics|       2|     2500|2025-06-09|\n",
            "|    110|     Lavanya|  Comic Book|      Books|       4|      200|2023-01-10|\n",
            "|    111|    Sereesha|       Shirt|   Clothing|       1|      700|2025-05-11|\n",
            "|    112|    Elakkiya|Dining Table|  Furniture|       2|     8000|2025-07-12|\n",
            "+-------+------------+------------+-----------+--------+---------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"ProductOrders\").getOrCreate()\n",
        "data = [\n",
        "    (101, \"Abinaya\", \"Smartphone\", \"Electronics\", 2, 15000, \"2025-07-01\"),\n",
        "    (102, \"Sereesha\", \"T-Shirt\", \"Clothing\", 3, 500, \"2025-06-02\"),\n",
        "    (103, \"Charu\", \"Laptop\", \"Electronics\", 1, 60000, \"2025-05-03\"),\n",
        "    (104, \"Harish\", \"Sofa\", \"Furniture\", 1, 25000, \"2025-07-04\"),\n",
        "    (105, \"Elakkiya\", \"Novel\", \"Books\", 2, 300, \"2025-07-05\"),\n",
        "    (106, \"Kashifa\", \"Tablet\", \"Electronics\", 1, 20000, \"2025-06-06\"),\n",
        "    (107, \"Roja\", \"Dress\", \"Clothing\", 2, 1500, \"2023-01-07\"),\n",
        "    (108, \"Harish\", \"Bookshelf\", \"Furniture\", 1, 8000, \"2025-07-08\"),\n",
        "    (109, \"Varshini\", \"Headphones\", \"Electronics\", 2, 2500, \"2025-06-09\"),\n",
        "    (110, \"Lavanya\", \"Comic Book\", \"Books\", 4, 200, \"2023-01-10\"),\n",
        "    (111, \"Sereesha\", \"Shirt\", \"Clothing\", 1, 700, \"2025-05-11\"),\n",
        "    (112, \"Elakkiya\", \"Dining Table\", \"Furniture\", 2, 8000, \"2025-07-12\")\n",
        "]\n",
        "columns = [\"OrderID\", \"CustomerName\", \"Product\", \"Category\", \"Quantity\", \"UnitPrice\", \"OrderDate\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"orders_local\")\n",
        "df.createOrReplaceGlobalTempView(\"orders_global\")"
      ],
      "metadata": {
        "id": "5_8XPv93W0TI"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part A: Local View – orders_local"
      ],
      "metadata": {
        "id": "qxDPw586XBRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. List all orders placed for \"Electronics\" with a Quantity of 2 or more.\n",
        "spark.sql(\"select * from orders_local where Category = 'Electronics' and Quantity >= 2 \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BaZQzQsctVQ",
        "outputId": "66bdb589-fb45-484f-d8d4-89d2ac2ab636"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+----------+-----------+--------+---------+----------+\n",
            "|OrderID|CustomerName|   Product|   Category|Quantity|UnitPrice| OrderDate|\n",
            "+-------+------------+----------+-----------+--------+---------+----------+\n",
            "|    101|     Abinaya|Smartphone|Electronics|       2|    15000|2025-07-01|\n",
            "|    109|    Varshini|Headphones|Electronics|       2|     2500|2025-06-09|\n",
            "+-------+------------+----------+-----------+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Calculate TotalAmount (Quantity × UnitPrice) for each order.\n",
        "spark.sql(\"select OrderID, (Quantity * UnitPrice) as TotalAmount from orders_local \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGwypBhWdAzo",
        "outputId": "a79c78fa-aa0c-471b-d368-8d3cc491aa4c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----------+\n",
            "|OrderID|TotalAmount|\n",
            "+-------+-----------+\n",
            "|    101|      30000|\n",
            "|    102|       1500|\n",
            "|    103|      60000|\n",
            "|    104|      25000|\n",
            "|    105|        600|\n",
            "|    106|      20000|\n",
            "|    107|       3000|\n",
            "|    108|       8000|\n",
            "|    109|       5000|\n",
            "|    110|        800|\n",
            "|    111|        700|\n",
            "|    112|      16000|\n",
            "+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Show the total number of orders per Category .\n",
        "spark.sql(\"select Category, count(*) as Total_no_orders from orders_local group by Category \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdGU2c9tdeRu",
        "outputId": "863778d9-58f1-4570-c903-8acf07a66d83"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+---------------+\n",
            "|   Category|Total_no_orders|\n",
            "+-----------+---------------+\n",
            "|Electronics|              4|\n",
            "|   Clothing|              3|\n",
            "|      Books|              2|\n",
            "|  Furniture|              3|\n",
            "+-----------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. List orders placed in \"January 2023\" only.\n",
        "spark.sql(\"select * from orders_local where OrderDate like '2023-01%'\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyQ_qvN_dtnp",
        "outputId": "a3974033-8e21-4bee-f679-d29efabca16e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+----------+--------+--------+---------+----------+\n",
            "|OrderID|CustomerName|   Product|Category|Quantity|UnitPrice| OrderDate|\n",
            "+-------+------------+----------+--------+--------+---------+----------+\n",
            "|    107|        Roja|     Dress|Clothing|       2|     1500|2023-01-07|\n",
            "|    110|     Lavanya|Comic Book|   Books|       4|      200|2023-01-10|\n",
            "+-------+------------+----------+--------+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Show the average UnitPrice per category.\n",
        "spark.sql(\"select Category, round(avg(UnitPrice),2) from orders_local group by Category\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wusu0LLnefk5",
        "outputId": "730b3d4f-6836-4b84-c796-7d13bd21fc77"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------------------+\n",
            "|   Category|round(avg(UnitPrice), 2)|\n",
            "+-----------+------------------------+\n",
            "|Electronics|                 24375.0|\n",
            "|   Clothing|                   900.0|\n",
            "|      Books|                   250.0|\n",
            "|  Furniture|                13666.67|\n",
            "+-----------+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Find the order with the highest total amount.\n",
        "spark.sql(\"select *,(Quantity * UnitPrice) as TotalAmount from orders_local order by TotalAmount Desc limit 1\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNVJeBSJfDw0",
        "outputId": "ca9b6ac9-a027-48fd-85c9-8d643665d80f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+-------+-----------+--------+---------+----------+-----------+\n",
            "|OrderID|CustomerName|Product|   Category|Quantity|UnitPrice| OrderDate|TotalAmount|\n",
            "+-------+------------+-------+-----------+--------+---------+----------+-----------+\n",
            "|    103|       Charu| Laptop|Electronics|       1|    60000|2025-05-03|      60000|\n",
            "+-------+------------+-------+-----------+--------+---------+----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Drop the local view and try querying it again.\n",
        "spark.catalog.dropTempView(\"orders_local\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fti8Voz8fq2E",
        "outputId": "d51a7ad4-41d2-4bb2-e024-40ce76cd5585"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# running querry after froppinf local view gives error\n",
        "spark.sql(\"select * from orders_local \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Lzf-iRPIf0UW",
        "outputId": "0bbf3ebb-e04a-4cf7-ec4c-75c28ef36cb8"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `orders_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [orders_local], [], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4155098057.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select * from orders_local \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0m_to_java_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                 )\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlitArgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `orders_local` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 14;\n'Project [*]\n+- 'UnresolvedRelation [orders_local], [], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part B: Global View – orders_global"
      ],
      "metadata": {
        "id": "9AFqscikcpKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Display all \"Furniture\" orders with TotalAmount above 10,000.\n",
        "spark.sql(\"select * from global_temp.orders_global where Category = 'Furniture' and Quantity * UnitPrice > 10000 \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TA4HJZtXIqo",
        "outputId": "13a98b64-88f3-4584-dab5-9626a0a54af9"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+------------+---------+--------+---------+----------+\n",
            "|OrderID|CustomerName|     Product| Category|Quantity|UnitPrice| OrderDate|\n",
            "+-------+------------+------------+---------+--------+---------+----------+\n",
            "|    104|      Harish|        Sofa|Furniture|       1|    25000|2025-07-04|\n",
            "|    112|    Elakkiya|Dining Table|Furniture|       2|     8000|2025-07-12|\n",
            "+-------+------------+------------+---------+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Create a column called DiscountFlag :\n",
        "# Mark \"Yes\" if Quantity > 3\n",
        "# Otherwise \"No\"\n",
        "from pyspark.sql.functions import when\n",
        "discount_df = df.withColumn('DiscountFlag', when(df.Quantity >= 3, 'Yes').otherwise('NO'))\n",
        "discount_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THzOa2SiYRM9",
        "outputId": "8452a6eb-d53a-4b08-d671-c1a4e180852e"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+------------+-----------+--------+---------+----------+------------+\n",
            "|OrderID|CustomerName|     Product|   Category|Quantity|UnitPrice| OrderDate|DiscountFlag|\n",
            "+-------+------------+------------+-----------+--------+---------+----------+------------+\n",
            "|    101|     Abinaya|  Smartphone|Electronics|       2|    15000|2025-07-01|          NO|\n",
            "|    102|    Sereesha|     T-Shirt|   Clothing|       3|      500|2025-06-02|         Yes|\n",
            "|    103|       Charu|      Laptop|Electronics|       1|    60000|2025-05-03|          NO|\n",
            "|    104|      Harish|        Sofa|  Furniture|       1|    25000|2025-07-04|          NO|\n",
            "|    105|    Elakkiya|       Novel|      Books|       2|      300|2025-07-05|          NO|\n",
            "|    106|     Kashifa|      Tablet|Electronics|       1|    20000|2025-06-06|          NO|\n",
            "|    107|        Roja|       Dress|   Clothing|       2|     1500|2023-01-07|          NO|\n",
            "|    108|      Harish|   Bookshelf|  Furniture|       1|     8000|2025-07-08|          NO|\n",
            "|    109|    Varshini|  Headphones|Electronics|       2|     2500|2025-06-09|          NO|\n",
            "|    110|     Lavanya|  Comic Book|      Books|       4|      200|2023-01-10|         Yes|\n",
            "|    111|    Sereesha|       Shirt|   Clothing|       1|      700|2025-05-11|          NO|\n",
            "|    112|    Elakkiya|Dining Table|  Furniture|       2|     8000|2025-07-12|          NO|\n",
            "+-------+------------+------------+-----------+--------+---------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. List customers who ordered more than 1 product type (Hint: use GROUP BY and HAVING).\n",
        "spark.sql(\"select CustomerName,count(*) from global_temp.orders_global group by CustomerName having count(*) > 1\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCuD3-z2ZXaK",
        "outputId": "6fafa973-3e96-4a6b-a3b8-d592e376e43c"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+--------+\n",
            "|CustomerName|count(1)|\n",
            "+------------+--------+\n",
            "|      Harish|       2|\n",
            "|    Sereesha|       2|\n",
            "|    Elakkiya|       2|\n",
            "+------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Count number of orders per month across the dataset.\n",
        "spark.sql(\"select substring(OrderDate, 1, 7) as Month, count(*) OrderCount from global_temp.orders_global group by Month order by Month\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRkcAbojaM32",
        "outputId": "aff64796-984e-4fa3-aca6-d83bd7f0326e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|  Month|OrderCount|\n",
            "+-------+----------+\n",
            "|2023-01|         2|\n",
            "|2025-05|         2|\n",
            "|2025-06|         3|\n",
            "|2025-07|         5|\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Rank all products by total quantity sold across all orders using a window function.\n",
        "from pyspark.sql.functions import sum, rank\n",
        "from pyspark.sql.window import Window\n",
        "product_totals = df.groupBy(\"Product\").agg(sum(\"Quantity\").alias(\"TotalQuantity\"))\n",
        "ranked = product_totals.withColumn(\"Rank\",rank().over(Window.orderBy(product_totals[\"TotalQuantity\"].desc())))\n",
        "ranked.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhZgfIhPbZCN",
        "outputId": "ddb35262-912c-49ea-89e2-5ff093f7225d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------+----+\n",
            "|     Product|TotalQuantity|Rank|\n",
            "+------------+-------------+----+\n",
            "|  Comic Book|            4|   1|\n",
            "|     T-Shirt|            3|   2|\n",
            "|       Novel|            2|   3|\n",
            "|  Smartphone|            2|   3|\n",
            "|       Dress|            2|   3|\n",
            "|Dining Table|            2|   3|\n",
            "|  Headphones|            2|   3|\n",
            "|      Laptop|            1|   8|\n",
            "|        Sofa|            1|   8|\n",
            "|      Tablet|            1|   8|\n",
            "|   Bookshelf|            1|   8|\n",
            "|       Shirt|            1|   8|\n",
            "+------------+-------------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Run a query using a new SparkSession and the global view.\n",
        "new_spark = SparkSession.builder.appName(\"ProductOrdersNew\").getOrCreate()"
      ],
      "metadata": {
        "id": "vhqxbMU1b25L"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# running querry after creating new session\n",
        "spark.sql(\"select * from global_temp.orders_global \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN2x8hrJgKjS",
        "outputId": "9ee93caf-5428-41a3-eb7e-181149046c1e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+------------+-----------+--------+---------+----------+\n",
            "|OrderID|CustomerName|     Product|   Category|Quantity|UnitPrice| OrderDate|\n",
            "+-------+------------+------------+-----------+--------+---------+----------+\n",
            "|    101|     Abinaya|  Smartphone|Electronics|       2|    15000|2025-07-01|\n",
            "|    102|    Sereesha|     T-Shirt|   Clothing|       3|      500|2025-06-02|\n",
            "|    103|       Charu|      Laptop|Electronics|       1|    60000|2025-05-03|\n",
            "|    104|      Harish|        Sofa|  Furniture|       1|    25000|2025-07-04|\n",
            "|    105|    Elakkiya|       Novel|      Books|       2|      300|2025-07-05|\n",
            "|    106|     Kashifa|      Tablet|Electronics|       1|    20000|2025-06-06|\n",
            "|    107|        Roja|       Dress|   Clothing|       2|     1500|2023-01-07|\n",
            "|    108|      Harish|   Bookshelf|  Furniture|       1|     8000|2025-07-08|\n",
            "|    109|    Varshini|  Headphones|Electronics|       2|     2500|2025-06-09|\n",
            "|    110|     Lavanya|  Comic Book|      Books|       4|      200|2023-01-10|\n",
            "|    111|    Sereesha|       Shirt|   Clothing|       1|      700|2025-05-11|\n",
            "|    112|    Elakkiya|Dining Table|  Furniture|       2|     8000|2025-07-12|\n",
            "+-------+------------+------------+-----------+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bonus Challenges"
      ],
      "metadata": {
        "id": "yEZxC2fngTQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Save a filtered subset (only \"Books\" category) as a new global temp view.\n",
        "df.filter(df.Category == \"Books\").createOrReplaceGlobalTempView(\"books_orders\")\n",
        "spark.sql(\"select * from global_temp.books_orders\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jqux2oGgWKf",
        "outputId": "7fa04b46-838f-4e69-b9f6-9624bedbca32"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+----------+--------+--------+---------+----------+\n",
            "|OrderID|CustomerName|   Product|Category|Quantity|UnitPrice| OrderDate|\n",
            "+-------+------------+----------+--------+--------+---------+----------+\n",
            "|    105|    Elakkiya|     Novel|   Books|       2|      300|2025-07-05|\n",
            "|    110|     Lavanya|Comic Book|   Books|       4|      200|2023-01-10|\n",
            "+-------+------------+----------+--------+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Find the most purchased product per category.\n",
        "product_totals = df.groupBy(\"Category\", \"Product\").agg(sum(\"Quantity\").alias(\"TotalQuantity\"))\n",
        "windowSpec = Window.partitionBy(\"Category\").orderBy(product_totals[\"TotalQuantity\"].desc())\n",
        "ranked = product_totals.withColumn(\"Rank\", rank().over(windowSpec))\n",
        "top_products = ranked.filter(ranked.Rank == 1)\n",
        "top_products.select(\"Category\", \"Product\", \"TotalQuantity\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFMTtEYAgf2h",
        "outputId": "0271d21f-0040-4dad-c58f-cce9cb6af6cc"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+-------------+\n",
            "|   Category|     Product|TotalQuantity|\n",
            "+-----------+------------+-------------+\n",
            "|      Books|  Comic Book|            4|\n",
            "|   Clothing|     T-Shirt|            3|\n",
            "|Electronics|  Smartphone|            2|\n",
            "|Electronics|  Headphones|            2|\n",
            "|  Furniture|Dining Table|            2|\n",
            "+-----------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create a view that excludes all \"Clothing\" orders and call it \"filtered_orders\" .\n",
        "df.filter(df.Category != \"Clothing\").createOrReplaceTempView(\"filtered_orders\")\n",
        "spark.sql(\"select * from filtered_orders\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCp6aZIpg_YF",
        "outputId": "84657055-087d-4743-b97c-4b5ccc186bed"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+------------+------------+-----------+--------+---------+----------+\n",
            "|OrderID|CustomerName|     Product|   Category|Quantity|UnitPrice| OrderDate|\n",
            "+-------+------------+------------+-----------+--------+---------+----------+\n",
            "|    101|     Abinaya|  Smartphone|Electronics|       2|    15000|2025-07-01|\n",
            "|    103|       Charu|      Laptop|Electronics|       1|    60000|2025-05-03|\n",
            "|    104|      Harish|        Sofa|  Furniture|       1|    25000|2025-07-04|\n",
            "|    105|    Elakkiya|       Novel|      Books|       2|      300|2025-07-05|\n",
            "|    106|     Kashifa|      Tablet|Electronics|       1|    20000|2025-06-06|\n",
            "|    108|      Harish|   Bookshelf|  Furniture|       1|     8000|2025-07-08|\n",
            "|    109|    Varshini|  Headphones|Electronics|       2|     2500|2025-06-09|\n",
            "|    110|     Lavanya|  Comic Book|      Books|       4|      200|2023-01-10|\n",
            "|    112|    Elakkiya|Dining Table|  Furniture|       2|     8000|2025-07-12|\n",
            "+-------+------------+------------+-----------+--------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}