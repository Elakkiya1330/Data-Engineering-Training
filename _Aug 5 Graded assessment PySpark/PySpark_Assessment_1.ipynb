{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. PySpark Setup & Initialization\n",
        "##Exercise 1.1 – Setup Spark:"
      ],
      "metadata": {
        "id": "jJ0t3ceHxCQy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "VnuDvrLkwsNr"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"BotCampus Intermediate Session\").master(\"local[*]\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 1.2 – Load starter data:"
      ],
      "metadata": {
        "id": "Qu2x1Ku3xFxs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Ananya\", \"Bangalore\", 24),\n",
        "(\"Ravi\", \"Hyderabad\", 28),\n",
        "(\"Kavya\", \"Delhi\", 22),\n",
        "(\"Meena\", \"Chennai\", 25)]\n",
        "columns = [\"name\", \"city\", \"age\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ngk2eUelw2HA",
        "outputId": "8e5276f9-35e2-4c0c-eebd-babf111e2b19"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+---------+---+\n",
            "|  name|     city|age|\n",
            "+------+---------+---+\n",
            "|Ananya|Bangalore| 24|\n",
            "|  Ravi|Hyderabad| 28|\n",
            "| Kavya|    Delhi| 22|\n",
            "| Meena|  Chennai| 25|\n",
            "+------+---------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. RDDs & Transformations\n",
        "##Exercise 2.1 – Create RDD from feedback:"
      ],
      "metadata": {
        "id": "dOcezshLxJzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feedback = spark.sparkContext.parallelize([\n",
        "\"Ravi from Bangalore loved the mobile app\",\n",
        "\"Meena from Delhi reported poor response time\",\n",
        "\"Ajay from Pune liked the delivery speed\",\n",
        "\"Ananya from Hyderabad had an issue with UI\",\n",
        "\"Rohit from Mumbai gave positive feedback\"\n",
        "])"
      ],
      "metadata": {
        "id": "h6wy1yZxxQMy"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count total number of words.\n",
        "word_count=feedback.flatMap(lambda line: line.split()).count()\n",
        "print(\"Total number of words:\", word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "635JgrZ5yELX",
        "outputId": "f5cad967-453e-4c3e-9102-e55bec7a0648"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of words: 35\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find top 3 most common words.\n",
        "word_count = (feedback.flatMap(lambda line: line.split( )).map(lambda w: (w.lower(), 1)).reduceByKey(lambda a, b: a + b))\n",
        "top3 = word_count.takeOrdered(3, key=lambda x: -x[1])\n",
        "print(\"Top 3 xommon words: \", top3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y9JMsrayQFn",
        "outputId": "9d25786c-d582-401b-d2f4-f08a599291d8"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 xommon words:  [('from', 5), ('the', 2), ('loved', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words ( from , with , the , etc.).\n",
        "stop_words = {\"from\", \"with\", \"the\", \"an\", \"and\", \"had\", \"gave\"}\n",
        "filtered_words = feedback.flatMap(lambda line: line.lower().split()).filter(lambda w: w not in stop_words)\n",
        "print(filtered_words.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUip8B_SzD3O",
        "outputId": "6d5afbcb-a69b-499f-e68b-c11252d9cd80"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ravi', 'bangalore', 'loved', 'mobile', 'app', 'meena', 'delhi', 'reported', 'poor', 'response', 'time', 'ajay', 'pune', 'liked', 'delivery', 'speed', 'ananya', 'hyderabad', 'issue', 'ui', 'rohit', 'mumbai', 'positive', 'feedback']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of word → count.\n",
        "word_dict = dict(word_count.collect())\n",
        "print(word_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-mmQlZ20eGg",
        "outputId": "6d35b4e9-347c-4f51-b2a1-89ebb0e124fe"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'from': 5, 'loved': 1, 'app': 1, 'poor': 1, 'response': 1, 'liked': 1, 'speed': 1, 'ananya': 1, 'an': 1, 'issue': 1, 'with': 1, 'rohit': 1, 'mumbai': 1, 'positive': 1, 'feedback': 1, 'ravi': 1, 'bangalore': 1, 'the': 2, 'mobile': 1, 'meena': 1, 'delhi': 1, 'reported': 1, 'time': 1, 'ajay': 1, 'pune': 1, 'delivery': 1, 'hyderabad': 1, 'had': 1, 'ui': 1, 'gave': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. DataFrames – Transformations\n",
        "##Exercise 3.1 – Create exam_scores DataFrame:"
      ],
      "metadata": {
        "id": "WiHJKe-x1Ffk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scores = [\n",
        "(\"Ravi\", \"Math\", 88),\n",
        "(\"Ananya\", \"Science\", 92),\n",
        "(\"Kavya\", \"English\", 79),\n",
        "(\"Ravi\", \"English\", 67),\n",
        "(\"Neha\", \"Math\", 94),\n",
        "(\"Meena\", \"Science\", 85)\n",
        "]\n",
        "columns = [\"name\", \"subject\", \"score\"]\n",
        "df_scores = spark.createDataFrame(scores, columns)"
      ],
      "metadata": {
        "id": "kyfiZPjy1FL2"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add grade column ( >=90 → A, 80-89 → B, 70-79 → C, else D).\n",
        "from pyspark.sql.functions import when\n",
        "df_scores = df_scores.withColumn(\"grade\", when(df_scores.score >= 90, \"A\").when((df_scores.score >= 80) & (df_scores.score < 90), \"B\")\n",
        ".when((df_scores.score >= 70) & (df_scores.score < 80), \"C\").otherwise(\"D\"))\n",
        "df_scores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykTJb_Fn1Ntb",
        "outputId": "e120e837-bbed-4759-b2a9-e7eca7817c1f"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+\n",
            "|  name|subject|score|grade|\n",
            "+------+-------+-----+-----+\n",
            "|  Ravi|   Math|   88|    B|\n",
            "|Ananya|Science|   92|    A|\n",
            "| Kavya|English|   79|    C|\n",
            "|  Ravi|English|   67|    D|\n",
            "|  Neha|   Math|   94|    A|\n",
            "| Meena|Science|   85|    B|\n",
            "+------+-------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by subject, find average score.\n",
        "from pyspark.sql.functions import avg\n",
        "df_scores.groupBy(\"subject\").avg(\"score\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGq4c5zq1vEd",
        "outputId": "a4698fae-7973-467c-a010-946031dc4348"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------+\n",
            "|subject|avg(score)|\n",
            "+-------+----------+\n",
            "|Science|      88.5|\n",
            "|   Math|      91.0|\n",
            "|English|      73.0|\n",
            "+-------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use when and otherwise to classify subject difficulty ( Math/Science = Difficult).\n",
        "df_scores = df_scores.withColumn(\"difficulty\", when(df_scores.subject.isin(\"Math\", \"Science\"), \"Difficult\").otherwise(\"Easy\"))\n",
        "df_scores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "027QSpsd177c",
        "outputId": "99f28e87-2147-4c48-9741-e0a9db1029bf"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+----------+\n",
            "|  name|subject|score|grade|difficulty|\n",
            "+------+-------+-----+-----+----------+\n",
            "|  Ravi|   Math|   88|    B| Difficult|\n",
            "|Ananya|Science|   92|    A| Difficult|\n",
            "| Kavya|English|   79|    C|      Easy|\n",
            "|  Ravi|English|   67|    D|      Easy|\n",
            "|  Neha|   Math|   94|    A| Difficult|\n",
            "| Meena|Science|   85|    B| Difficult|\n",
            "+------+-------+-----+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rank students per subject using Window function.\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "window_spec = Window.partitionBy(\"subject\").orderBy(df_scores.score.desc())\n",
        "df_scores = df_scores.withColumn(\"rank\", rank().over(window_spec))\n",
        "df_scores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0Tm86Y32ZEI",
        "outputId": "e18abf6a-916b-4280-d22a-00b25e48f93f"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+----------+----+\n",
            "|  name|subject|score|grade|difficulty|rank|\n",
            "+------+-------+-----+-----+----------+----+\n",
            "| Kavya|English|   79|    C|      Easy|   1|\n",
            "|  Ravi|English|   67|    D|      Easy|   2|\n",
            "|  Neha|   Math|   94|    A| Difficult|   1|\n",
            "|  Ravi|   Math|   88|    B| Difficult|   2|\n",
            "|Ananya|Science|   92|    A| Difficult|   1|\n",
            "| Meena|Science|   85|    B| Difficult|   2|\n",
            "+------+-------+-----+-----+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply UDF to format names (e.g., make all uppercase).\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "def upper_name(name):\n",
        "    return name.upper()\n",
        "upper_udf = udf(upper_name, StringType())\n",
        "df_scores = df_scores.withColumn(\"name\", upper_udf(df_scores.name))\n",
        "df_scores.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Cd-hOkL4Nnf",
        "outputId": "dd8f9793-5406-453a-9bbc-3e873ab2a073"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+----------+----+\n",
            "|  name|subject|score|grade|difficulty|rank|\n",
            "+------+-------+-----+-----+----------+----+\n",
            "| KAVYA|English|   79|    C|      Easy|   1|\n",
            "|  RAVI|English|   67|    D|      Easy|   2|\n",
            "|  NEHA|   Math|   94|    A| Difficult|   1|\n",
            "|  RAVI|   Math|   88|    B| Difficult|   2|\n",
            "|ANANYA|Science|   92|    A| Difficult|   1|\n",
            "| MEENA|Science|   85|    B| Difficult|   2|\n",
            "+------+-------+-----+-----+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Ingest CSV & JSON – Save to Parquet\n",
        "##Dataset 1: CSV file: students.csv"
      ],
      "metadata": {
        "id": "m7G5Rfn64lGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "students_data = \"\"\"id,name,department,city,salary\n",
        "1,Amit,IT,Bangalore,78000\n",
        "2,Kavya,HR,Chennai,62000\n",
        "3,Arjun,Finance,Hyderabad,55000\"\"\"\n",
        "\n",
        "with open('students.csv', 'w') as f:\n",
        "  f.write(students_data)\n",
        "\n"
      ],
      "metadata": {
        "id": "sBgDFFXD4QrG"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV with header\n",
        "df_csv = spark.read.option(\"header\", True).csv(\"students.csv\", inferSchema=True)\n",
        "df_csv.printSchema()\n",
        "df_csv.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGxoexP6AILx",
        "outputId": "48e3fc9a-7945-4fe2-ec4a-d9728554ef34"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- department: string (nullable = true)\n",
            " |-- city: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n",
            "+---+-----+----------+---------+------+\n",
            "| id| name|department|     city|salary|\n",
            "+---+-----+----------+---------+------+\n",
            "|  1| Amit|        IT|Bangalore| 78000|\n",
            "|  2|Kavya|        HR|  Chennai| 62000|\n",
            "|  3|Arjun|   Finance|Hyderabad| 55000|\n",
            "+---+-----+----------+---------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_data = \"\"\"[\n",
        "  {\n",
        "    \"id\": 101,\n",
        "    \"name\": \"Sneha\",\n",
        "    \"address\": {\n",
        "      \"city\": \"Mumbai\",\n",
        "      \"pincode\": 400001\n",
        "    },\n",
        "    \"skills\": [\"Python\", \"Spark\"]\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "with open('employee_nested.json', 'w') as f:\n",
        "  f.write(json_data)"
      ],
      "metadata": {
        "id": "kT52JbAcANGB"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load nested JSON\n",
        "df_json = spark.read.option(\"multiline\", True).json(\"employee_nested.json\")\n",
        "df_json.printSchema()\n",
        "df_json.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q1hf4l0rAibT",
        "outputId": "6f59000e-56e1-4978-d96f-ba7fb9988772"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- pincode: long (nullable = true)\n",
            " |-- id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- skills: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+----------------+---+-----+---------------+\n",
            "|         address| id| name|         skills|\n",
            "+----------------+---+-----+---------------+\n",
            "|{Mumbai, 400001}|101|Sneha|[Python, Spark]|\n",
            "+----------------+---+-----+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the JSON (use explode , select , alias ).\n",
        "from pyspark.sql.functions import explode, col\n",
        "df_flat = df_json.select(\"id\",\"name\",col(\"address.city\").alias(\"city\"),col(\"address.pincode\").alias(\"pincode\"),explode(\"skills\").alias(\"skill\"))\n",
        "df_flat.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjVYPoeuA30u",
        "outputId": "4f3dfc53-94fd-466d-a5bf-aa1ae0e0deb1"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+-------+------+\n",
            "| id| name|  city|pincode| skill|\n",
            "+---+-----+------+-------+------+\n",
            "|101|Sneha|Mumbai| 400001|Python|\n",
            "|101|Sneha|Mumbai| 400001| Spark|\n",
            "+---+-----+------+-------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert both to Parquet and write to /tmp/output .\n",
        "df_csv.write.mode(\"overwrite\").parquet(\"/tmp/output/students_parquet\")\n",
        "df_flat.write.mode(\"overwrite\").parquet(\"/tmp/output/employees_parquet\")"
      ],
      "metadata": {
        "id": "iJQTpx9UBNK0"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Spark SQL – Temp Views & Queries\n",
        "##Exercise 5.1 Create view from exam scores and run:"
      ],
      "metadata": {
        "id": "_zKwCIuyBWrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_scores.createOrReplaceTempView(\"exam_scores\")"
      ],
      "metadata": {
        "id": "t7tNUskpBTu8"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a) Top scorer per subject\n",
        "spark.sql(\"select subject , max(score) as top_score from exam_scores group by subject order by top_score Desc \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZnzXXctCNAi",
        "outputId": "8cfc8263-3172-4769-db70-906099a2e864"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|subject|top_score|\n",
            "+-------+---------+\n",
            "|   Math|       94|\n",
            "|Science|       92|\n",
            "|English|       79|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# b) Count of students per grade\n",
        "spark.sql(\"select grade , count(*) as no_of_students from exam_scores group by grade order by grade \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q6GjLueC4FS",
        "outputId": "8447eaf0-8cf0-4666-da05-d46b77487c0e"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------+\n",
            "|grade|no_of_students|\n",
            "+-----+--------------+\n",
            "|    A|             2|\n",
            "|    B|             2|\n",
            "|    C|             1|\n",
            "|    D|             1|\n",
            "+-----+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# c) Students with multiple subjects\n",
        "spark.sql(\"select name , count(distinct subject) as no_of_subjects from exam_scores group by name having no_of_subjects > 1 \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5zdEAOHDMsX",
        "outputId": "de9c058f-7a18-4c2a-bdec-d32c3328b074"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------+\n",
            "|name|no_of_subjects|\n",
            "+----+--------------+\n",
            "|RAVI|             2|\n",
            "+----+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# d) Subjects with average score above 85\n",
        "spark.sql(\"select subject , avg(score) as avg_score from exam_scores group by subject having avg_score > 85 \").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPtarnBUDqrr",
        "outputId": "4cd7ec0b-5c74-48f1-9201-8626507ba88d"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---------+\n",
            "|subject|avg_score|\n",
            "+-------+---------+\n",
            "|Science|     88.5|\n",
            "|   Math|     91.0|\n",
            "+-------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 5.2 Create another DataFrame attendance(name, days_present) and:"
      ],
      "metadata": {
        "id": "iZK9j1RmElF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attendance_data = [\n",
        "    (\"RAVI\", 18),\n",
        "    (\"ANANYA\", 22),\n",
        "    (\"KAVYA\", 20),\n",
        "    (\"NEHA\", 19),\n",
        "    (\"MEENA\", 23)\n",
        "]\n",
        "columns = [\"name\", \"days_present\"]\n",
        "df_attendance = spark.createDataFrame(attendance_data, columns)"
      ],
      "metadata": {
        "id": "TJa1nrOrEnSP"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join with scores\n",
        "df_joined = df_scores.join(df_attendance, on=\"name\", how=\"left\")\n",
        "df_joined.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EkQxbLDE0tH",
        "outputId": "f2462c6c-ca0b-4e77-a6a3-7f5b2f655094"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+----------+----+------------+\n",
            "|  name|subject|score|grade|difficulty|rank|days_present|\n",
            "+------+-------+-----+-----+----------+----+------------+\n",
            "|  NEHA|   Math|   94|    A| Difficult|   1|          19|\n",
            "| MEENA|Science|   85|    B| Difficult|   2|          23|\n",
            "|  RAVI|English|   67|    D|      Easy|   2|          18|\n",
            "|  RAVI|   Math|   88|    B| Difficult|   2|          18|\n",
            "|ANANYA|Science|   92|    A| Difficult|   1|          22|\n",
            "| KAVYA|English|   79|    C|      Easy|   1|          20|\n",
            "+------+-------+-----+-----+----------+----+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate attendance-adjusted grade:\n",
        "# If days_present < 20 → downgrade grade by one level\n",
        "df_adjust = df_joined.withColumn(\"adj_grade\", when((df_joined.days_present < 20) & (df_joined.grade == \"A\"), \"B\")\n",
        ".when((df_joined.days_present < 20) & (df_joined.grade == \"B\"), \"C\").when((df_joined.days_present < 20) & (df_joined.grade == \"C\"), \"D\")\n",
        ".otherwise(df_joined.grade))\n",
        "\n",
        "df_adjust.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mExstkS1FXMg",
        "outputId": "095eb6ff-f16e-4906-b832-f35f31e68b90"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+-----+-----+----------+----+------------+---------+\n",
            "|  name|subject|score|grade|difficulty|rank|days_present|adj_grade|\n",
            "+------+-------+-----+-----+----------+----+------------+---------+\n",
            "|  NEHA|   Math|   94|    A| Difficult|   1|          19|        B|\n",
            "| MEENA|Science|   85|    B| Difficult|   2|          23|        B|\n",
            "|  RAVI|English|   67|    D|      Easy|   2|          18|        D|\n",
            "|  RAVI|   Math|   88|    B| Difficult|   2|          18|        C|\n",
            "|ANANYA|Science|   92|    A| Difficult|   1|          22|        A|\n",
            "| KAVYA|English|   79|    C|      Easy|   1|          20|        C|\n",
            "+------+-------+-----+-----+----------+----+------------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Partitioned Load (Full + Incremental)"
      ],
      "metadata": {
        "id": "d-1XgU_iGtE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial Load:\n",
        "df_scores.write.partitionBy(\"subject\").parquet(\"/tmp/scores/\", mode = 'overwrite')\n",
        "incremental = [(\"Meena\", \"Math\", 93)]\n",
        "columns = [\"name\", \"subject\", \"score\"]\n",
        "df_inc = spark.createDataFrame(incremental, columns)\n",
        "df_inc = df_inc.withColumn(\"grade\", when(df_inc.score >= 90, \"A\").when((df_inc.score >= 80) & (df_inc.score < 90), \"B\")\n",
        ".when((df_inc.score >= 70) & (df_inc.score < 80), \"C\").otherwise(\"D\"))\n",
        "df_inc.write.mode(\"append\").partitionBy(\"subject\").parquet(\"/tmp/scores/\")"
      ],
      "metadata": {
        "id": "I19B6EIbGqg3"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List all folders inside /tmp/scores/\n",
        "df_all = spark.read.parquet(\"/tmp/scores/\")\n",
        "df_all.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBiI-VOnJp99",
        "outputId": "90ec0b06-09ae-4201-f2f6-ce95c8670257"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-----+----------+----+-------+\n",
            "|  name|score|grade|difficulty|rank|subject|\n",
            "+------+-----+-----+----------+----+-------+\n",
            "|ANANYA|   92|    A| Difficult|   1|Science|\n",
            "| MEENA|   85|    B| Difficult|   2|Science|\n",
            "|  NEHA|   94|    A| Difficult|   1|   Math|\n",
            "|  RAVI|   88|    B| Difficult|   2|   Math|\n",
            "| KAVYA|   79|    C|      Easy|   1|English|\n",
            "|  RAVI|   67|    D|      Easy|   2|English|\n",
            "| Meena|   93|    A|      NULL|NULL|   Math|\n",
            "+------+-----+-----+----------+----+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read only Math partition and display all entries.\n",
        "df_math = spark.read.parquet(\"/tmp/scores/subject=Math\")\n",
        "df_math.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVEL_fMgKIEJ",
        "outputId": "4dc89d46-9f80-4ef9-e574-e841df8f8c15"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----+----------+----+\n",
            "| name|score|grade|difficulty|rank|\n",
            "+-----+-----+-----+----------+----+\n",
            "| NEHA|   94|    A| Difficult|   1|\n",
            "| RAVI|   88|    B| Difficult|   2|\n",
            "|Meena|   93|    A|      NULL|NULL|\n",
            "+-----+-----+-----+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. ETL: Clean, Transform, Load"
      ],
      "metadata": {
        "id": "GpW7G1KOKxE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"\"\"emp_id,name,dept,salary,bonus\n",
        "1,Arjun,IT,78000,5000\n",
        "2,Kavya,HR,62000,\n",
        "3,Sneha,Finance,55000,3000\"\"\"\n",
        "with open('employee_data.csv','w') as f:\n",
        "  f.write(data)"
      ],
      "metadata": {
        "id": "Sb8CouyfKwxb"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data with header.\n",
        "df = spark.read.option(\"header\", True).csv(\"employee_data.csv\", inferSchema=True)\n",
        "df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_87kav6zK1U_",
        "outputId": "dc3def8f-5390-4274-884d-106992e977c6"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+\n",
            "|emp_id| name|   dept|salary|bonus|\n",
            "+------+-----+-------+------+-----+\n",
            "|     1|Arjun|     IT| 78000| 5000|\n",
            "|     2|Kavya|     HR| 62000| NULL|\n",
            "|     3|Sneha|Finance| 55000| 3000|\n",
            "+------+-----+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill missing bonus with 2000.\n",
        "df = df.fillna({\"bonus\" : 2000})\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2qaj6epLk5P",
        "outputId": "7c8f9c6b-ff00-4b8f-c150-612f2304ba79"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+\n",
            "|emp_id| name|   dept|salary|bonus|\n",
            "+------+-----+-------+------+-----+\n",
            "|     1|Arjun|     IT| 78000| 5000|\n",
            "|     2|Kavya|     HR| 62000| 2000|\n",
            "|     3|Sneha|Finance| 55000| 3000|\n",
            "+------+-----+-------+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total_ctc = salary + bonus .\n",
        "from pyspark.sql.functions import col\n",
        "df = df.withColumn('total_ctc', col('salary') + col('bonus'))\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkBit7CHMLZs",
        "outputId": "c58ae6e9-cbc7-4790-ef40-93d69d06b7f5"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+-------+------+-----+---------+\n",
            "|emp_id| name|   dept|salary|bonus|total_ctc|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "|     1|Arjun|     IT| 78000| 5000|    83000|\n",
            "|     2|Kavya|     HR| 62000| 2000|    64000|\n",
            "|     3|Sneha|Finance| 55000| 3000|    58000|\n",
            "+------+-----+-------+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter where total_ctc > 60,000.\n",
        "df_filter = df.filter(col(\"total_ctc\") > 60000)\n",
        "df_filter.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpASj1P3Mpca",
        "outputId": "c7804ab4-7836-40f5-9d13-06b58a98ca61"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+----+------+-----+---------+\n",
            "|emp_id| name|dept|salary|bonus|total_ctc|\n",
            "+------+-----+----+------+-----+---------+\n",
            "|     1|Arjun|  IT| 78000| 5000|    83000|\n",
            "|     2|Kavya|  HR| 62000| 2000|    64000|\n",
            "+------+-----+----+------+-----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save final DataFrame to Parquet and JSON.\n",
        "# i am saving the filterd data\n",
        "df_filter.write.mode(\"overwrite\").parquet(\"/tmp/final_employees_parquet\")\n",
        "df_filter.write.mode(\"overwrite\").json(\"/tmp/final_employees_json\")"
      ],
      "metadata": {
        "id": "Cw6EpjTOM3bM"
      },
      "execution_count": 117,
      "outputs": []
    }
  ]
}