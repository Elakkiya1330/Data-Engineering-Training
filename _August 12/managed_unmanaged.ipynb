{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_2-_rRZupPPA"
      },
      "outputs": [],
      "source": [
        "!pip install delta-spark==3.2.0 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from delta import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create a SparkSession with Delta Lake extensions\n",
        "# The '.config(...)' lines are crucial for enabling Delta Lake's features\n",
        "builder = pyspark.sql.SparkSession.builder.appName(\"DeltaTutorial\") \\\n",
        "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
        "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
        "\n",
        "# Get or create the SparkSession\n",
        "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
        "\n",
        "print(\"Spark and Delta Lake are ready!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIYqu2fGpQzv",
        "outputId": "d7fe822f-933f-4b1a-e430-eab7c91d4bd9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark and Delta Lake are ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# managed\n",
        "data = [('elakkiya' , 21),('varshini',22),('sanju', 20)]\n",
        "df = spark.createDataFrame(data,['Name','Age'])\n",
        "# save in delta format\n",
        "df.write.format(\"delta\").saveAsTable('managed_people')\n",
        "# show the table\n",
        "spark.sql('select * from managed_people').show()\n",
        "# location is inside warehouse(managed by spark)\n",
        "location = spark.sql('DESCRIBE DETAIL managed_people').collect()[0]['location']\n",
        "print(location)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH9kOE9upXA7",
        "outputId": "89f545c6-8d7e-45e0-81a7-5c836e01cb43"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+---+\n",
            "|    Name|Age|\n",
            "+--------+---+\n",
            "|elakkiya| 21|\n",
            "|varshini| 22|\n",
            "|   sanju| 20|\n",
            "+--------+---+\n",
            "\n",
            "file:/content/spark-warehouse/managed_people\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('drop table managed_people')\n",
        "# Deletes files and table info."
      ],
      "metadata": {
        "id": "lN2oGfMCpxB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unmanaged\n",
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/MyDrive/students.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df = spark.createDataFrame(df)\n",
        "df.write.option(\"path\", \"/content/drive/MyDrive/people_data\").saveAsTable('unmanaged_data')\n",
        "spark.sql('select * from unmanaged_data').show()\n",
        "location = spark.sql('DESCRIBE DETAIL unmanaged_data').collect()[0]['location']\n",
        "print(location)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qabBYHruqWnV",
        "outputId": "d95f2134-6b91-4d04-ad28-aae0fb9d28aa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+-------+-------+\n",
            "| Name|Math|Science|English|\n",
            "+-----+----+-------+-------+\n",
            "|Kiran|  65|     70|     60|\n",
            "|Anita|  88|     95|     90|\n",
            "| Ravi|  55|     60|     58|\n",
            "| Amit|  78|     85|     74|\n",
            "|Priya|  92|     89|     96|\n",
            "+-----+----+-------+-------+\n",
            "\n",
            "file:/content/drive/MyDrive/people_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('drop table unmanaged_data')\n",
        "# Deletes only metadata, files stay in Drive."
      ],
      "metadata": {
        "id": "DH2ZCiPpraP0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}